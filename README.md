# Pytorch_Autoencoder_Example
Autoencoder which acts same as traditional 3 bit encoder

## 0. Experiment Motivation
While taking a class EEE6431(Neural Networks), professor mentioned that autoencoder with one hidden layer which traiend based on deep-learning techniques acts exactly same as 3 bit traditional encoder.

Following is traditional 3-bit encoder truth table.
|input|output|
|------|---|
|00000001|000|
|00000010|001|
|00000100|010|
|00001000|011|
|00010000|100|
|00100000|101|
|01000000|110|
|10000000|111|

I was curious about whether it truly acts as a traditional encoder, so I designed a simple experiment using Pytorch. The experiment was conducted with sheer curiosity. 

## 1. Model structure
Following is a network representation.

![autoencoder_diagram](https://user-images.githubusercontent.com/77431192/117440740-6c694100-af6f-11eb-8169-47354897c160.png)

Sigmoid function is put between input layer and hidden layer as a activation function. At the output layer, softmax function is applied. 

## 2. Prepare Data
Data are already saved as `data.pickle`. The file comprises 8 kinds of one-hot encoded 8 bit data, number of data is 1000 for each kind. 
(details are below)


[1,0,0,0,0,0,0,0] x 1000,
[0,1,0,0,0,0,0,0] x 1000,
.
.
.
[0,0,0,0,0,0,0,1] x 1000

The file is generated by `generate_one_hot_data.py`.

## 3. Train
The model mentioned above is trained on classification task with Crossentropyloss. 
~~~
python train.py
~~~
Pretrained model are kept in `model_ckpt/Autoencoder` directory. 

## 4. Test
~~~
python test.py
~~~

Test results are as follows.
### 4.1. Raw results
![test_result_1](https://user-images.githubusercontent.com/77431192/117442286-8441c480-af71-11eb-8a7f-b519d78d0850.png)
### 4.2. Processesd results
* encoded : `torch.round` applied
* decoded : `torch.argmax` applied

![test_results_2](https://user-images.githubusercontent.com/77431192/117442294-86a41e80-af71-11eb-8239-98d3e71be439.PNG)

